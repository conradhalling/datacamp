{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "381a263e-b6d1-488e-840b-a00f141323c7",
   "metadata": {},
   "source": [
    "# Introduction to Importing Data in Python\n",
    "\n",
    "These are my notes for DataCamp's course [_Introduction to Importing Data in Python_](https://www.datacamp.com/courses/introduction-to-importing-data-in-python).\n",
    "\n",
    "This course is presented by Hugo Bowne-Anderson, formerly Data Scientist at DataCamp. The collaborator is Francisco Castro.\n",
    "\n",
    "Prerequisite:\n",
    "\n",
    "- [_Intermediate Python_](../Intermediate%20Python/Intermediate%20Python.ipynb)\n",
    "\n",
    "This course is part of these tracks:\n",
    "\n",
    "- Data Engineer\n",
    "- Scientist with Python\n",
    "- Data Scientist Professional with Python\n",
    "- Importing & Cleaning Data with Python\n",
    "\n",
    "## Data Sets\n",
    "\n",
    "| Name | File |\n",
    "|:---|:---|\n",
    "| Chinook (SQLite) | Chinook.sqlite |\n",
    "| LIGO (HDF5) | L-L1_LOSC_4_V1-1126259446-32.hdf5 |\n",
    "| Battledeath (XLSX) | battledeath.xlsx |\n",
    "| Extent of Infectious Diseases | disarea.dta |\n",
    "| Gene expressions (Matlab) | ja_data2.mat |\n",
    "| MNIST | mnist_kaggle_some_rows.csv |\n",
    "| Sales (SA7SBDAT) | sales.sas7bdat |\n",
    "| Seaslugs | seaslugs.txt |\n",
    "| Titanic | titanic_sub.csv |\n",
    "\n",
    "## Imports\n",
    "\n",
    "For convenience and clarity, all imports are gathered here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe1bde1-cb08-474f-b0b3-b9f19ac5e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sqlite3\n",
    "import sys\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sas7bdat\n",
    "import scipy.io\n",
    "import sqlalchemy\n",
    "\n",
    "# Display all warnings as errors.\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"error\"\n",
    "\n",
    "# Set a default plotting style.\n",
    "plt.style.use(\"dark_background\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a885417-d2f4-44dc-9fd6-f74066586c97",
   "metadata": {},
   "source": [
    "## Introduction and Flat Files\n",
    "\n",
    "### Welcome to the Course\n",
    "\n",
    "#### Reading a Text File (Demonstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e9c7e7-4ca4-4e08-968d-602a290d0107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a text file.\n",
    "filename = 'seaslug.txt'\n",
    "file1 = open(filename, mode=\"r\")\n",
    "seaslug_text = file1.read()\n",
    "print(seaslug_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f8f175-c7a4-4aa3-8f08-9c233a962819",
   "metadata": {},
   "source": [
    "#### Checking if a File is Closed (Extra)\n",
    "\n",
    "This won't matter when we move on to using `with`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ec323f-4145-4da9-b11d-8bc3ef3df612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is possible to determine whether a file is closed.\n",
    "print(\"File is closed:\", file1.closed)\n",
    "print(\"Closing file...\")\n",
    "file1.close()\n",
    "print(\"  Done.\")\n",
    "print(\"File is closed:\", file1.closed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8549dbf-3348-45ef-8179-83829add3e60",
   "metadata": {},
   "source": [
    "#### Writing to a File (Demonstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d52448-c247-4829-a86c-b2464d95258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to a file. I have enhanced this demonstration.\n",
    "# Create the \"moby_dick.txt\" file, which is used below in an exercise.\n",
    "moby_dick_text = \"\"\"CHAPTER 1. Loomings.\n",
    "\n",
    "Call me Ishmael. Some years ago--never mind how long precisely--having\n",
    "little or no money in my purse, and nothing particular to interest me on\n",
    "shore, I thought I would sail about a little and see the watery part of\n",
    "the world. It is a way I have of driving off the spleen and regulating\n",
    "the circulation. Whenever I find myself growing grim about the mouth;\n",
    "whenever it is a damp, drizzly November in my soul; whenever I find\n",
    "myself involuntarily pausing before coffin warehouses, and bringing up\n",
    "the rear of every funeral I meet; and especially whenever my hypos get\n",
    "such an upper hand of me, that it requires a strong moral principle to\n",
    "prevent me from deliberately stepping into the street, and methodically\n",
    "knocking people's hats off--then, I account it high time to get to sea\n",
    "as soon as I can. This is my substitute for pistol and ball. With a\n",
    "philosophical flourish Cato throws himself upon his sword; I quietly\n",
    "take to the ship. There is nothing surprising in this. If they but knew\n",
    "it, almost all men in their degree, some time or other, cherish very\n",
    "nearly the same feelings towards the ocean with me.\n",
    "\"\"\"\n",
    "filename2 = 'moby_dick.txt'\n",
    "file2 = open(filename2, mode=\"w\")\n",
    "file2.write(moby_dick_text)\n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564c62ab-d7aa-42de-93a7-6ac653a29eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the file context manager, it is not necessary to call file.close().\n",
    "# Since this is not a function, file and text2 have global scope.\n",
    "with open(filename, mode=\"r\") as file3:\n",
    "    text2 = file3.read()\n",
    "    print(\"File is closed:\", file3.closed)\n",
    "print(file3)\n",
    "print(\"File is closed:\", file3.closed)\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c47e945-edf4-4e0a-b6cc-fa68ce988916",
   "metadata": {},
   "source": [
    "#### Explore the Working Directory (Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9594614c-e3d3-409a-b6ac-d3a54dd39e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the current working directory in IPython.\n",
    "# This can't be done in the regular Python interpreter.\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03adf365-4a07-46c3-ad68-185f0d7c9745",
   "metadata": {},
   "source": [
    "#### Read an Entire Text File (Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaa8bd5-1eba-470b-b0ae-096b87369b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and print the entire file \"moby_dick.txt\".\n",
    "file4 = open(\"moby_dick.txt\", mode=\"r\")\n",
    "print(file4.read())\n",
    "print(\"File is closed:\", file4.closed)\n",
    "print(\"Closing file...\")\n",
    "file4.close()\n",
    "print(\"  Done.\")\n",
    "print(\"File is closed:\", file4.closed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fe19cb-f0b4-449c-9a39-3e8d2170fa17",
   "metadata": {},
   "source": [
    "#### Importing Text Files Line By Line (Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506bc220-ebd9-4c39-969d-6bffb7c2d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use file.readline() to read a file line by line.\n",
    "with open(\"moby_dick.txt\", mode=\"r\") as file5:\n",
    "    print(file5.readline())\n",
    "    print(file5.readline())\n",
    "    print(file5.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0f727b-2db1-4da5-966b-ec8a26225a34",
   "metadata": {},
   "source": [
    "### The Importance of Flat Files in Data Science\n",
    "\n",
    "Flat files may contain a header followed by rows of data, where each row contains the attributes for a single object. [In bioinformatics, flat files often have other internal organization.] Hugo contrasts flat files with relational database tables; a flat file contains no relational information.\n",
    "\n",
    "Hugo is using the titanic.csv file as an example of a CSV flat file.\n",
    "\n",
    "MNIST.txt is a tab-delimited text file.\n",
    "> The data consists of the famous MNIST digit recognition images, where each row contains the pixel values of a given image. Note that all fields in the MNIST data are numeric.\n",
    "\n",
    "Typically, a data scientist uses NumPy or pandas to work with flat files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c85dbb2-fb88-42d3-90e0-6ba2baae46d6",
   "metadata": {},
   "source": [
    "#### Characteristics of Flat Files (Exercise)\n",
    "- Flat files consist of rows, and each row is called a record.\n",
    "- A record in a flat file is composed of _fields_ or _attributes_, each one of which contains at most one item of information.\n",
    "- Flat files are pervasive in data science."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e38bec-df9e-496b-b5f9-1c1fd2f9a5d9",
   "metadata": {},
   "source": [
    "#### Why We Like Flat Files and the Zen of Python (Exercise)\n",
    "\n",
    "The fifth aphorism of _The Zen of Python_ is: \"Flat is better than nested.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3726dbb6-aa0f-4ccd-9440-b116b2711076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain The Zen of Python.\n",
    "import this\n",
    "# See https://stackoverflow.com/questions/5855758/what-is-the-source-code-of-the-this-module-doing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad49927-4919-4b30-8732-7b855f0c6f84",
   "metadata": {},
   "source": [
    "### Importing Flat Files with NumPy\n",
    "\n",
    "If all the data in a flat file are numerical, read the file using NumPy. NumPy arrays are the standard for storing numerical data. NumPy arrays are often used by other data science packages such as scikit-learn. NumPy itself has a number of built-in functions that make it far easier and more efficient for us to import data as arrays.\n",
    "- `numpy.loadtext()`\n",
    "- `numpy.genfromtext()`\n",
    "- `numpy.recfromcsv()`\n",
    "\n",
    "To modify how NumPy imports the data:\n",
    "- Use the `skiprows=1` argument to skip the header row.\n",
    "- Use the usecols argument to select the columns to keep (e.g., `usecols=[0, 2]`).\n",
    "- Use the dtype argument to specify the data type (e.g., `dtype=str`).\n",
    "\n",
    "NumPy does not handle mixed data types well, such as the data that appear in the titanic_sub.csv file. To import such data, use `pandas.read_csv()` to create a pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1574c41-f4aa-416c-8aa0-1c130ef018b6",
   "metadata": {},
   "source": [
    "#### Read the MNIST Digit Data (Exercise)\n",
    "\n",
    "See http://yann.lecun.com/exdb/mnist/ for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb77c16-23bd-4298-b799-e6b80712aa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the MNIST data.\n",
    "filename = 'mnist_kaggle_some_rows.csv'\n",
    "digits = np.loadtxt(filename, delimiter=',')\n",
    "print(\"Type of object digits:\", type(digits))\n",
    "\n",
    "# Select and reshape a row. The first value is the label (the digit the image\n",
    "# represents.) The remaining 28 * 28 values represent a 28 x 28 image.\n",
    "# Reshape and present the data from row 21.\n",
    "digit = int(digits[21, 0])\n",
    "print(\"digit:\", digit)\n",
    "im = digits[21, 1:]\n",
    "im_sq = np.reshape(im, (28, 28))\n",
    "\n",
    "# Plot reshaped data.\n",
    "plt.imshow(im_sq, cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff46ae0f-50af-45d5-86d2-0cd837ccf756",
   "metadata": {},
   "source": [
    "#### Customizing Your NumPy Import (Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482f048c-289d-4315-82c3-b3d97d7b59ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used the !cat digits_header.txt file to print the contents of the file\n",
    "# to the DataCamp console. I copied the data and saved it as\n",
    "# digits_header.txt.\n",
    "# The file has a header in the first row; we can't include the header in the\n",
    "# NumPy array. For the exercise, keep only columns 0 and 2, where column 0\n",
    "# is the encoded digit and column 2 is a pixel value. Since both are integers,\n",
    "# set the dtype of the data to int.\n",
    "file = 'digits_header.txt'\n",
    "data = np.loadtxt(file, delimiter=\"\\t\", skiprows=1, usecols=[0, 2], dtype=int)\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f797db-37d8-40dd-9ed7-589bed1e3f85",
   "metadata": {},
   "source": [
    "#### Importing Different Datatypes (Exercise)\n",
    "\n",
    "These data consist of the percentage of sea slug larvae that had metamorphosed in a given time period. See http://www.stat.ucla.edu/projects/datasets/seaslug-explanation.html for the sea slug data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f21f874-e04f-41ff-8760-9ed01f39583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file has string headers and numeric fields. This can't be loaded using np.loadtxt() because\n",
    "# of inconsistent data types, unless we set dtype=str to convert all data to strings, or we\n",
    "# set skiprows=1 to skip the header row.\n",
    "file = 'seaslug.txt'\n",
    "data = np.loadtxt(file, delimiter='\\t', dtype=str)\n",
    "print(data[:2])\n",
    "\n",
    "# Import data as floats and skip the first row.\n",
    "data_float = np.loadtxt(file, delimiter='\\t', dtype=float, skiprows=1)\n",
    "print(data_float[9])\n",
    "\n",
    "# Plot a scatterplot of the data.\n",
    "plt.scatter(data_float[:, 0], data_float[:, 1])\n",
    "plt.xlabel('Time (min)')\n",
    "plt.ylabel('Percentage of larvae')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9840cc93-b0ca-4e98-9ce5-f1b35308aea4",
   "metadata": {},
   "source": [
    "#### Working with Mixed Datatypes (Structured Arrays) (Exercise)\n",
    "\n",
    "`nunpy.loadtext` can't load data having different datatypes; use `numpy.genfromtxt` for this.\n",
    "\n",
    "`numpy.genfromtxt` creates a structured array object, https://numpy.org/doc/stable/user/basics.rec.html. Each row in the structured array has type `numpy.void`; see https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.void."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a4d091-09d0-4cbd-b7e8-c75ff3839f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with mixed datatypes (1)\n",
    "# Use np.genfromtxt() instead of np.loadtxt() because it can process mixed\n",
    "# datatypes. np.genfromtxt returns a structured numpy.ndarray.\n",
    "# Because numpy arrays have to contain elements that are all the\n",
    "# same type, the structured array solves this by being a 1-D array, where each\n",
    "# element in the array is a row of the flat file imported.\n",
    "\n",
    "# If the encoding argument is not specified, numpy creates a warning:\n",
    "#   Reading unicode strings without specifying the encoding argument is\n",
    "#   deprecated. Set the encoding, use None for the system default.\n",
    "data = np.genfromtxt('titanic_sub.csv', delimiter=',', names=True, dtype=None, encoding=None)\n",
    "print(\"type(data):\", type(data))\n",
    "print(\"data.shape:\", data.shape)\n",
    "# numpy.ndarray.dtype has attributes names and fields.\n",
    "print(\"data.dtype:\", data.dtype)\n",
    "print(\"data.dtype.names:\", data.dtype.names)\n",
    "print(\"data.dtype.fields:\", data.dtype.fields)\n",
    "print(\"sys.getsizeof(data):\", sys.getsizeof(data))\n",
    "print(\"row 0:\", data[0])\n",
    "# The type of row[0] is numpy.void, whatever that is.\n",
    "print(\"type(data[0]):\", type(data[0]))\n",
    "print(\"sys.getsizeof(data[0]):\", sys.getsizeof(data[0]))\n",
    "print('\"Fare\" column, first five items:')\n",
    "print(data[\"Fare\"][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35f9c12-221c-4bb0-b1bb-4ed9242d03f1",
   "metadata": {},
   "source": [
    "#### Working with Mixed Datatypes (Record Arrays) (Exercise)\n",
    "\n",
    "The `numpy.recfromcsv` function returns `numpy.recarray` objects; see https://numpy.org/doc/stable/user/basics.rec.html#record-arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa9c932-65d4-454e-93fa-7c6b0ba4261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use np.recfromcsv for reading CSV files.\n",
    "# Print the first three rows of data.\n",
    "file = 'titanic_sub.csv'\n",
    "d = np.recfromcsv(file, encoding=None)\n",
    "print(\"type(d):\", type(d))\n",
    "print(\"d.dtype:\", d.dtype)\n",
    "print(\"d.shape:\", d.shape)\n",
    "print(d[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c837d05-0010-4e0b-8b3e-1602b91b428e",
   "metadata": {},
   "source": [
    "### Importing Flat Files Using Pandas\n",
    "\n",
    "NumPy is not adequate for dealing with tables of data. It is standard practice\n",
    "and best practice to use pandas. See https://pandas.pydata.org.\n",
    "\n",
    "Quoted from the course:\n",
    "\n",
    "> Although arrays are incredibly powerful and serve a number of essential purposes, they cannot fulfill one of the most basic needs of a Data Scientist: to have \"[two]-dimensional labeled data structure[s] with columns of potentially different types\" that you can easily perform a plethora of Data Sciencey type things on: manipulate, slice, reshaped, groupby, join, merge, perform statistics in a missing-value-friendly manner, deal with times series. The need for such a data structure, among other issues, prompted Wes McKinney to develop the pandas library for Python. Nothing speaks to the project of pandas more than the documentation itself: \"Python has long been great for data munging and preparation, but less so for data analysis and modeling. pandas helps fill this gap, enabling you to carry out your entire data analysis workflow in Python without having to switch to a more domain specific language like R.\" The data structure most relevant to the data manipulation and analysis workflow that pandas offers is the dataframe and it is the Pythonic analogue of R’s dataframe.\n",
    "\n",
    "> Manipulating dataframes in pandas can be useful in all steps of the data scientific method, from exploratory data analysis to data wrangling, preprocessing, building models and visualization. Here we will see its great utility in importing flat files, even merely in the way that it deals with missing data, comments along with the many other issues that plague working data scientists. For all of these reasons, it is now standard and best practice in Data Science to use pandas to import flat files as dataframes. Later in this course, we’ll see how many other types of data, whether they’re stored in relational databases, hdf5, MATLAB or excel files, can easily be imported as dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4084e46-f3dc-44e9-8ba3-d755567c185b",
   "metadata": {},
   "source": [
    "#### Using pandas to Import Flat Files as DataFrames (1) (Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752eaa1b-9ae3-4780-b96f-406bd2c706a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Titanic data using pandas.read_csv() to create a\n",
    "# pandas.DataFrame object.\n",
    "filename = 'titanic_sub.csv'\n",
    "titanic = pd.read_csv(filename)\n",
    "print(titanic.head())\n",
    "print(\"type(titanic):\", type(titanic))\n",
    "\n",
    "# Extract a NumPy array from the DataFrame.\n",
    "titanic_values = titanic.values\n",
    "print(\"type(titanic_values):\", type(titanic_values))\n",
    "print(\"titanic_values.dtype:\", titanic_values.dtype)\n",
    "print(titanic_values[:5])\n",
    "print(\"type(titanic_values[0]):\", type(titanic_values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b377d6f0-45dc-4b2c-9ea2-2dc413d0e97d",
   "metadata": {},
   "source": [
    "#### Using pandas to Import Flat Files as DataFrames (2) (Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741dda40-b7b7-4dae-9fad-7d81e5187abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the first five rows of the file into a DataFrame, where the file does not\n",
    "# contain a header. Extract the data as a numpy.ndarray object.\n",
    "file = 'mnist_kaggle_some_rows.csv'\n",
    "mnist = pd.read_csv(file, nrows=5, header=None)\n",
    "mnist_array = mnist.values\n",
    "print(type(mnist_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e33763f-6ef1-4fce-a36f-cc31e6834d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used the command !cat titanic_corrupt.txt in the DataCamp console, copied\n",
    "# the data from the console, and pasted it into a new text file,\n",
    "# titanic_corrupt.txt, in the project folder.\n",
    "# I wrote the code when I didn't have a coy of titanic_corrupt.txt, hence\n",
    "# the use of try..except.\n",
    "# The file is tab-delimited, contains comments starting with '#', and indicates\n",
    "# missing data with the string 'Nothing'.\n",
    "# This code shows how to use pandas to plot a histogram.\n",
    "file = 'titanic_corrupt.txt'\n",
    "try:\n",
    "    data = pd.read_csv(file, sep='\\t', comment='#', na_values='Nothing')\n",
    "    print(data.head())\n",
    "    pd.DataFrame.hist(data[['Age']])\n",
    "    plt.xlabel('Age (years)')\n",
    "    plt.ylabel('count')\n",
    "    plt.show()\n",
    "except Exception as ex:\n",
    "    print(\"I don't have the 'titanic_corrupt.txt' file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a16f03b-c45b-4320-bc3c-ea4950c4e27e",
   "metadata": {},
   "source": [
    "## Importing Other File Types\n",
    "\n",
    "### Introduction to Other File Types\n",
    "\n",
    "Hugo Bowne-Anderson discussed different file formats, including Python's pickle format. HDF5 files are increasing used for storing large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c608bee5-1835-41d0-9f18-3eb8f94cfb6d",
   "metadata": {},
   "source": [
    "#### List Directories in IPython (Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6413746-a348-4b86-b262-f2bd5c3d74f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the current working directory.\n",
    "cwd = os.getcwd()\n",
    "print(os.listdir(cwd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1c4304-e947-41f1-aeff-c2cbf72f5dfd",
   "metadata": {},
   "source": [
    "#### Dump and Load a Pickled File (Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed29545-7306-42ad-b371-5c933d213e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pickled file. This replicates the file used in the exercise.\n",
    "d = {'Mar': '84.4', 'June': '69.4', 'Aug': '85', 'Airline': '8'}\n",
    "filename = 'data.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(d, file)\n",
    "\n",
    "# Read the pickled file.\n",
    "with open(filename, 'rb') as file:\n",
    "    d2 = pickle.load(file)\n",
    "print(d2)\n",
    "print(type(d2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b073a5c-092e-4f78-9334-815d058f81c7",
   "metadata": {},
   "source": [
    "#### List Worksheets in an Excel Files (Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ab3baa-45bf-4f41-97c3-c31c8bf55783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This requires installing openpyxl for Excel support.\n",
    "file = 'battledeath.xlsx'\n",
    "# The parse method creates a pandas.core.frame.DataFrame from a sheet.\n",
    "xl = pd.ExcelFile(file)\n",
    "print(\"type(xl):\", type(xl))\n",
    "print(xl.sheet_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c035b3-e8e7-42df-a7b8-dc7f16872afc",
   "metadata": {},
   "source": [
    "#### Load Data from Excel Worksheets (Exercise)\n",
    "\n",
    "For documentation, see https://pandas.pydata.org/docs/reference/api/pandas.ExcelFile.parse.html and https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d13817-ab98-4abb-8c34-802f7bf438b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the individual worksheets into DataFrames.\n",
    "for sheet_name in xl.sheet_names:\n",
    "    worksheet = xl.parse(sheet_name)\n",
    "    print(worksheet.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a280edc-1b9e-45d6-80ea-26be9a1e571c",
   "metadata": {},
   "source": [
    "#### Customize Excel Import (Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f90992b-0894-48ed-8ea3-a2e14cbaada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parse xl.parse (pd.io.excel.ExcelFile.parse method takes arguments such\n",
    "# as skiprows, names, and parse_cols. But the documentation is inadequate.\n",
    "# Documentation for the arguments is available from pd.read_excel.\n",
    "file = 'battledeath.xlsx'\n",
    "xl = pd.ExcelFile(file)\n",
    "print(type(xl))\n",
    "print()\n",
    "\n",
    "# Parse the first sheet, skip the header row, and rename the columns.\n",
    "df1 = xl.parse(0, skiprows=[0], names=['Country', 'AAM due to War (2002)'])\n",
    "print(df1.head())\n",
    "print()\n",
    "\n",
    "# Parse the second sheet, keeping only the first column, skipping the header\n",
    "# row, and setting the name of the single column.\n",
    "df2 = xl.parse(1, usecols=[0], skiprows=[0], names=['Country'])\n",
    "print(df2.head())\n",
    "print()\n",
    "\n",
    "# This is from a review exercise.\n",
    "# Here the skiprows argument does not include the header line.\n",
    "df3 = xl.parse(1, skiprows=1, names=[\"Countries\", \"Death\"])\n",
    "print(df3.head())\n",
    "print()\n",
    "\n",
    "# This is from a review exercise.\n",
    "# Here the skiprows argument includes the header line.\n",
    "# The result is not something I would want.\n",
    "df4 = xl.parse(\"2002\", skiprows=1)\n",
    "print(df4.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411810c8-c9ca-41df-a779-82272ca75bec",
   "metadata": {},
   "source": [
    "### Importing SAS or Stata Files Using pandas\n",
    "\n",
    "SAS stands for \"Statistical Analysis System\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508a5eda-72ef-476e-9788-dc95427dcbb9",
   "metadata": {},
   "source": [
    "#### Importing SAS Files (Exercise)\n",
    "\n",
    "Use `sas7bdat.SAS7BDAT()` or `pandas.read_sas()` to import data from SAS files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59e8d82-59ff-4589-930d-75aae702cc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from a SAS file.\n",
    "with sas7bdat.SAS7BDAT('sales.sas7bdat') as file:\n",
    "    # file is a sas7bdat.SAS7BDAT object.\n",
    "    print(type(file))\n",
    "    df_sas = file.to_data_frame()\n",
    "print(df_sas.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f248b5-8cae-4fda-a908-98961839c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas has a read_sas method.\n",
    "df_sas2 = pd.read_sas('sales.sas7bdat')\n",
    "print(df_sas2.head())\n",
    "\n",
    "# Plot a histogram of DataFrame features.\n",
    "pd.DataFrame.hist(df_sas[['P']])\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8f1393-d403-478a-8684-a663b5cc2f2e",
   "metadata": {},
   "source": [
    "#### Importing Stata Files Using pandas (Exercise)\n",
    "\n",
    "Stata stands for \"Statistics data\". Use `pandas.read_stata()` to import data from a Stata file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2a4d58-f174-4719-8dc5-0307a4d0f6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data and create a histogram from one column.\n",
    "df = pd.read_stata('disarea.dta')\n",
    "print(df.head())\n",
    "pd.DataFrame.hist(df[['disa10']])\n",
    "plt.xlabel('Extend of disease')\n",
    "plt.ylabel('Number of countries')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8647819-70d2-4e96-a5d5-2b96f4befac2",
   "metadata": {},
   "source": [
    "### Importing HDF5 Files\n",
    "\n",
    "Hierarchical Data Format version 5 (HDF5) is rapidly becoming the standard for storing large quantities of numerical data. (See O'Reilly book, _Python and HDF5_.) HDF5 can handle datasets of sizes hundreds of gigabytes or terabyes, even exabytes. HDF5 is managed by the HDF Group in Champaign, IL.\n",
    "\n",
    "The example data comes from LIGO (Laster Interferometry Gravitational Wave Observatory) project. See https://losc.ligo.org/events/GW150914/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bef080-f8fa-4bd4-8a62-36324e4b20c1",
   "metadata": {},
   "source": [
    "#### Load HDF5 Data (Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a54721f-3a35-486c-9724-7b1cc6767f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HDF5 data.\n",
    "filename = 'L-L1_LOSC_4_V1-1126259446-32.hdf5'\n",
    "data = h5py.File(filename, 'r')\n",
    "# data has type h5py._hl.files.File. The data are hierarchical, not tabular.\n",
    "print(type(data))\n",
    "\n",
    "# Iterate through the structure: meta, quality, strain.\n",
    "# See http://h5py.org/, http://docs.h5py.org/.\n",
    "# We see:\n",
    "#   h5py._hl.group.Group\n",
    "#   h5py._hl.dataset.Dataset\n",
    "for key1 in data.keys():\n",
    "    print(key1, type(data[key1]))\n",
    "    for key2 in data[key1].keys():\n",
    "        print(\"  \", key2, type(data[key1][key2]))\n",
    "\n",
    "print()\n",
    "print(data['meta']['Description'], data['meta']['Detector'])\n",
    "print(np.array(data[\"meta\"][\"Description\"]))\n",
    "print(np.array(data[\"meta\"][\"Detector\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8312b35-1d23-47ae-a767-6b196ac1a9e2",
   "metadata": {},
   "source": [
    "#### Extract Data from HDF5 File (Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ff4875-f12c-496d-9809-e4b1fdece1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['strain'] is a Group.\n",
    "# data['strain']['Strain'] is a Dataset.\n",
    "# strain is a numpy.ndarray.\n",
    "strain = np.array(data['strain']['Strain'])\n",
    "print(\"type(strain):\", type(strain))\n",
    "print(strain.shape)\n",
    "\n",
    "# Plot the first 10,000 readings.\n",
    "num_samples = 10000\n",
    "time = np.arange(0, 1, 1/num_samples)\n",
    "plt.plot(time, strain[:num_samples])\n",
    "plt.xlabel('GPS time (s)')\n",
    "plt.ylabel('Strain')\n",
    "plt.show()\n",
    "\n",
    "# Plot all the data.\n",
    "# There are 131,072 values in strain.\n",
    "time = np.arange(0, len(strain)/10000, 1/10000)\n",
    "plt.plot(time, strain[:])\n",
    "plt.xlabel('GPS time (s)')\n",
    "plt.ylabel('Strain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745646f7-cf29-4d58-8409-516b03606642",
   "metadata": {},
   "source": [
    "### Importing MATLAB Files\n",
    "\n",
    "MATLAB is short for Matrix Laboratory. Use scipy to read and write MATLAB files:\n",
    "```Python\n",
    "scipy.io.loadmat()\n",
    "scipy.io.savemat()\n",
    "```\n",
    "\n",
    "The file contains gene expression data. See https://www.mcb.ucdavis.edu/faculty-labs/albeck/workshop.htm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7db948-54df-4b25-aa9f-ddc402ae9e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the dictionary, the keys are the names of the MATLAB variables\n",
    "# and the values are the values of the MATLAB variables.\n",
    "# Examine the shapes of the various datasets.\n",
    "filename = 'ja_data2.mat'\n",
    "mat = scipy.io.loadmat(filename)\n",
    "print(type(mat))\n",
    "\n",
    "# Look at the keys and values.\n",
    "for key, value in mat.items():\n",
    "    if isinstance(value, np.ndarray):\n",
    "        print(key, \":\", type(value), \":\", value.shape)\n",
    "    else:\n",
    "        print(key, \":\", type(value), \":\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f39d0a-80b2-4918-bdc1-36ae6ce1400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the array and plot it.\n",
    "print(type(mat['CYratioCyt']))\n",
    "print(mat['CYratioCyt'].shape)\n",
    "# Alternative, for the shape:\n",
    "print(np.shape(mat['CYratioCyt']))\n",
    "\n",
    "# Subset and plot the data for a single row.\n",
    "data = mat['CYratioCyt'][25, 5:]\n",
    "print(data)\n",
    "# fig = plt.figure()\n",
    "plt.plot(data)\n",
    "plt.xlabel('Time (min.)')\n",
    "plt.ylabel('Normalized fluoresence (measure of expression)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3177b6-bbc9-47ed-abd0-0aa4bb78c62f",
   "metadata": {},
   "source": [
    "## Working with Relational Databases in Python\n",
    "\n",
    "### Introduction to Relational Databases\n",
    "\n",
    "For Codd's twelve Rules, see https://en.wikipedia.org/wiki/Codd%27s_12_rules.\n",
    "\n",
    "In these exercises, we will use SQLAlchemy.\n",
    "\n",
    "The Northwind database, SQLite version, is available at https://github.com/jpwhite3/northwind-SQLite3. I copied the Northwind_small.sqlite file into the working directory. \n",
    "\n",
    "I found a comment on the web that the Chinook database, which is provided by the course, stores different data and is an improved version for testing. The Chinook database is available at https://github.com/lerocha/chinook-database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ac5cee-5889-477e-b3cb-1a78b3be6bac",
   "metadata": {},
   "source": [
    "#### Explore SQLite Databases Using the CLI (Extra)\n",
    "```\n",
    "$ sqlite3 Chinook.sqlite\n",
    "SQLite version 3.37.2 2022-01-06 13:25:41\n",
    "Enter \".help\" for usage hints.\n",
    "sqlite> .tables\n",
    "Album          Employee       InvoiceLine    PlaylistTrack\n",
    "Artist         Genre          MediaType      Track\n",
    "Customer       Invoice        Playlist\n",
    "sqlite> .quit\n",
    "\n",
    "$ sqlite3 Northwind_small.sqlite\n",
    "SQLite version 3.37.2 2022-01-06 13:25:41\n",
    "Enter \".help\" for usage hints.\n",
    "sqlite> .tables\n",
    "Category              EmployeeTerritory     Region\n",
    "Customer              Order                 Shipper\n",
    "CustomerCustomerDemo  OrderDetail           Supplier\n",
    "CustomerDemographic   Product               Territory\n",
    "Employee              ProductDetails_V\n",
    "sqlite> .mode html\n",
    "sqlite> select * from sqlite_schema;\n",
    "```\n",
    "\n",
    "I had to modify the HTML to present it the way I wanted. For some reason, HTML mode did not include the column names for the query.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>type</th>\n",
    "        <th>name</th>\n",
    "        <th>tbl_name</th>\n",
    "        <th>rootpage</th>\n",
    "        <th>sql</th>\n",
    "    </tr>\n",
    "<TR><TD>table</TD>\n",
    "<TD>Employee</TD>\n",
    "<TD>Employee</TD>\n",
    "<TD>2</TD>\n",
    "<TD>CREATE TABLE &quot;Employee&quot;<br>\n",
    "(<br>\n",
    "  &quot;Id&quot; INTEGER PRIMARY KEY,<br>\n",
    "  &quot;LastName&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;FirstName&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;Title&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;TitleOfCourtesy&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;BirthDate&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;HireDate&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;Address&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;City&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;Region&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;PostalCode&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;Country&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;HomePhone&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;Extension&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;Photo&quot; BLOB NULL,<br>\n",
    "  &quot;Notes&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;ReportsTo&quot; INTEGER NULL,<br>\n",
    "  &quot;PhotoPath&quot; VARCHAR(8000) NULL<br>\n",
    ")</TD>\n",
    "</TR>\n",
    "<TR><TD>table</TD>\n",
    "<TD>Category</TD>\n",
    "<TD>Category</TD>\n",
    "<TD>3</TD>\n",
    "<TD>CREATE TABLE &quot;Category&quot;<br>\n",
    "(<br>\n",
    "  &quot;Id&quot; INTEGER PRIMARY KEY,<br>\n",
    "  &quot;CategoryName&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;Description&quot; VARCHAR(8000) NULL<br>\n",
    ")</TD>\n",
    "</TR>\n",
    "<TR><TD>table</TD>\n",
    "<TD>Customer</TD>\n",
    "<TD>Customer</TD>\n",
    "<TD>4</TD>\n",
    "<TD>CREATE TABLE &quot;Customer&quot;<br>\n",
    "(<br>\n",
    "  &quot;Id&quot; VARCHAR(8000) PRIMARY KEY,<br>\n",
    "  &quot;CompanyName&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;ContactName&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;ContactTitle&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;Address&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;City&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;Region&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;PostalCode&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;Country&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;Phone&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;Fax&quot; VARCHAR(8000) NULL<br>\n",
    ")</TD>\n",
    "</TR>\n",
    "<TR><TD>index</TD>\n",
    "<TD>sqlite_autoindex_Customer_1</TD>\n",
    "<TD>Customer</TD>\n",
    "<TD>5</TD>\n",
    "<TD></TD>\n",
    "</TR>\n",
    "<TR><TD>table</TD>\n",
    "<TD>Shipper</TD>\n",
    "<TD>Shipper</TD>\n",
    "<TD>8</TD>\n",
    "<TD>CREATE TABLE &quot;Shipper&quot;<br>\n",
    "(<br>\n",
    "  &quot;Id&quot; INTEGER PRIMARY KEY,<br>\n",
    "  &quot;CompanyName&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;Phone&quot; VARCHAR(8000) NULL<br>\n",
    ")</TD>\n",
    "</TR>\n",
    "<TR><TD>table</TD>\n",
    "<TD>Supplier</TD>\n",
    "<TD>Supplier</TD>\n",
    "<TD>9</TD>\n",
    "<TD>CREATE TABLE &quot;Supplier&quot;<br>\n",
    "(<br>\n",
    "  &quot;Id&quot; INTEGER PRIMARY KEY,<br>\n",
    "  &quot;CompanyName&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;ContactName&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;ContactTitle&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;Address&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;City&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;Region&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;PostalCode&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;Country&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;Phone&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;Fax&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;HomePage&quot; VARCHAR(8000) NULL<br>\n",
    ")</TD>\n",
    "</TR>\n",
    "<TR><TD>table</TD>\n",
    "<TD>Order</TD>\n",
    "<TD>Order</TD>\n",
    "<TD>11</TD>\n",
    "<TD>CREATE TABLE &quot;Order&quot;<br>\n",
    "(<br>\n",
    "  &quot;Id&quot; INTEGER PRIMARY KEY,<br>\n",
    "  &quot;CustomerId&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;EmployeeId&quot; INTEGER NOT NULL,<br>\n",
    "  &quot;OrderDate&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;RequiredDate&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;ShippedDate&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;ShipVia&quot; INTEGER NULL,<br>\n",
    "  &quot;Freight&quot; DECIMAL NOT NULL,<br>\n",
    "  &quot;ShipName&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;ShipAddress&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;ShipCity&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;ShipRegion&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;ShipPostalCode&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;ShipCountry&quot; VARCHAR(8000) NULL<br>\n",
    ")</TD>\n",
    "</TR>\n",
    "<TR><TD>table</TD>\n",
    "<TD>Product</TD>\n",
    "<TD>Product</TD>\n",
    "<TD>12</TD>\n",
    "<TD>CREATE TABLE &quot;Product&quot;<br>\n",
    "(<br>\n",
    "  &quot;Id&quot; INTEGER PRIMARY KEY,<br>\n",
    "  &quot;ProductName&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;SupplierId&quot; INTEGER NOT NULL,<br>\n",
    "  &quot;CategoryId&quot; INTEGER NOT NULL,<br>\n",
    "  &quot;QuantityPerUnit&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;UnitPrice&quot; DECIMAL NOT NULL,<br>\n",
    "  &quot;UnitsInStock&quot; INTEGER NOT NULL,<br>\n",
    "  &quot;UnitsOnOrder&quot; INTEGER NOT NULL,<br>\n",
    "  &quot;ReorderLevel&quot; INTEGER NOT NULL,<br>\n",
    "  &quot;Discontinued&quot; INTEGER NOT NULL<br>\n",
    ")</TD>\n",
    "</TR>\n",
    "<TR><TD>table</TD>\n",
    "<TD>OrderDetail</TD>\n",
    "<TD>OrderDetail</TD>\n",
    "<TD>14</TD>\n",
    "<TD>CREATE TABLE &quot;OrderDetail&quot;<br>\n",
    "(<br>\n",
    "  &quot;Id&quot; VARCHAR(8000) PRIMARY KEY,<br>\n",
    "  &quot;OrderId&quot; INTEGER NOT NULL,<br>\n",
    "  &quot;ProductId&quot; INTEGER NOT NULL,<br>\n",
    "  &quot;UnitPrice&quot; DECIMAL NOT NULL,<br>\n",
    "  &quot;Quantity&quot; INTEGER NOT NULL,<br>\n",
    "  &quot;Discount&quot; DOUBLE NOT NULL<br>\n",
    ")</TD>\n",
    "</TR>\n",
    "<TR><TD>index</TD>\n",
    "<TD>sqlite_autoindex_OrderDetail_1</TD>\n",
    "<TD>OrderDetail</TD>\n",
    "<TD>15</TD>\n",
    "<TD></TD>\n",
    "</TR>\n",
    "<TR><TD>table</TD>\n",
    "<TD>CustomerCustomerDemo</TD>\n",
    "<TD>CustomerCustomerDemo</TD>\n",
    "<TD>16</TD>\n",
    "<TD>CREATE TABLE &quot;CustomerCustomerDemo&quot;<br>\n",
    "(<br>\n",
    "  &quot;Id&quot; VARCHAR(8000) PRIMARY KEY,<br>\n",
    "  &quot;CustomerTypeId&quot; VARCHAR(8000) NULL<br>\n",
    ")</TD>\n",
    "</TR>\n",
    "<TR><TD>index</TD>\n",
    "<TD>sqlite_autoindex_CustomerCustomerDemo_1</TD>\n",
    "<TD>CustomerCustomerDemo</TD>\n",
    "<TD>17</TD>\n",
    "<TD></TD>\n",
    "</TR>\n",
    "<TR><TD>table</TD>\n",
    "<TD>CustomerDemographic</TD>\n",
    "<TD>CustomerDemographic</TD>\n",
    "<TD>18</TD>\n",
    "<TD>CREATE TABLE &quot;CustomerDemographic&quot;<br>\n",
    "(<br>\n",
    "  &quot;Id&quot; VARCHAR(8000) PRIMARY KEY,<br>\n",
    "  &quot;CustomerDesc&quot; VARCHAR(8000) NULL<br>\n",
    ")</TD>\n",
    "</TR>\n",
    "<TR><TD>index</TD>\n",
    "<TD>sqlite_autoindex_CustomerDemographic_1</TD>\n",
    "<TD>CustomerDemographic</TD>\n",
    "<TD>19</TD>\n",
    "<TD></TD>\n",
    "</TR>\n",
    "<TR><TD>table</TD>\n",
    "<TD>Region</TD>\n",
    "<TD>Region</TD>\n",
    "<TD>21</TD>\n",
    "<TD>CREATE TABLE &quot;Region&quot;<br>\n",
    "(<br>\n",
    "  &quot;Id&quot; INTEGER PRIMARY KEY,<br>\n",
    "  &quot;RegionDescription&quot; VARCHAR(8000) NULL<br>\n",
    ")</TD>\n",
    "</TR>\n",
    "<TR><TD>table</TD>\n",
    "<TD>Territory</TD>\n",
    "<TD>Territory</TD>\n",
    "<TD>22</TD>\n",
    "<TD>CREATE TABLE &quot;Territory&quot;<br>\n",
    "(<br>\n",
    "  &quot;Id&quot; VARCHAR(8000) PRIMARY KEY,<br>\n",
    "  &quot;TerritoryDescription&quot; VARCHAR(8000) NULL,<br>\n",
    "  &quot;RegionId&quot; INTEGER NOT NULL<br>\n",
    ")</TD>\n",
    "</TR>\n",
    "<TR><TD>index</TD>\n",
    "<TD>sqlite_autoindex_Territory_1</TD>\n",
    "<TD>Territory</TD>\n",
    "<TD>23</TD>\n",
    "<TD></TD>\n",
    "</TR>\n",
    "<TR><TD>table</TD>\n",
    "<TD>EmployeeTerritory</TD>\n",
    "<TD>EmployeeTerritory</TD>\n",
    "<TD>24</TD>\n",
    "<TD>CREATE TABLE &quot;EmployeeTerritory&quot;<br>\n",
    "(<br>\n",
    "  &quot;Id&quot; VARCHAR(8000) PRIMARY KEY,<br>\n",
    "  &quot;EmployeeId&quot; INTEGER NOT NULL,<br>\n",
    "  &quot;TerritoryId&quot; VARCHAR(8000) NULL<br>\n",
    ")</TD>\n",
    "</TR>\n",
    "<TR><TD>index</TD>\n",
    "<TD>sqlite_autoindex_EmployeeTerritory_1</TD>\n",
    "<TD>EmployeeTerritory</TD>\n",
    "<TD>25</TD>\n",
    "<TD></TD>\n",
    "</TR>\n",
    "<TR><TD>view</TD>\n",
    "<TD>ProductDetails_V</TD>\n",
    "<TD>ProductDetails_V</TD>\n",
    "<TD>0</TD>\n",
    "<TD>CREATE VIEW [ProductDetails_V] as<br>\n",
    "select<br>\n",
    "p.*,<br>\n",
    "c.CategoryName, c.Description as [CategoryDescription],<br>\n",
    "s.CompanyName as [SupplierName], s.Region as [SupplierRegion]<br>\n",
    "from [Product] p<br>\n",
    "join [Category] c on p.CategoryId = c.id<br>\n",
    "join [Supplier] s on s.id = p.SupplierId</TD>\n",
    "</TR>\n",
    "</table>\n",
    "\n",
    "```\n",
    "sqlite>\n",
    "sqlite> .quit\n",
    "$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e929f6-dd3a-467d-92b4-008471cfa5fe",
   "metadata": {},
   "source": [
    "#### Explore SQLite Databases Using the sqlite3 Module (Extra)\n",
    "\n",
    "For documentation of the sqlite3 package, see https://docs.python.org/3/library/sqlite3.html.\n",
    "\n",
    "From https://www.sqlite.org/cli.html#querying_the_database_schema, this is a query that obtains the names of the tables:\n",
    "\n",
    "```SQL\n",
    "SELECT name FROM sqlite_schema \n",
    "WHERE type IN ('table','view') AND name NOT LIKE 'sqlite_%'\n",
    "ORDER BY name\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530f7639-c49a-4b36-9311-e30a7bcec4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the table names from the database.\n",
    "conn = sqlite3.connect(\"Northwind_small.sqlite\")\n",
    "cur = conn.cursor()\n",
    "sql1 = \"\"\"\n",
    "    SELECT\n",
    "        name\n",
    "    FROM\n",
    "        sqlite_schema \n",
    "    WHERE\n",
    "        type IN ('table', 'view')\n",
    "        AND name NOT LIKE 'sqlite_%'\n",
    "    ORDER BY\n",
    "        name\n",
    "\"\"\"\n",
    "cur.execute(sql1)\n",
    "rows = cur.fetchall() # a list of tuples\n",
    "print(rows)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c04d88a-aac6-4eda-821d-9c788dcc8b5c",
   "metadata": {},
   "source": [
    "### Creating a Database Engine in Python\n",
    "\n",
    "The course uses SQLAlchemy because it can be used to connect to many different relational database management systems.\n",
    "\n",
    "#### Get the Table Names Using sqlalchemy (Demonstration)\n",
    "\n",
    "The SQLAlchemy connection string is `sqlite:///Northwind_small.sqlite`. All three `/` characters are required for a relative path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fa32a9-cef8-49fc-a0f6-994d04ac95c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sqlalchemy to connect to the SQLite database.\n",
    "# This is the from the course's demonstration.\n",
    "engine = sqlalchemy.create_engine('sqlite:///Northwind_small.sqlite', future=True)\n",
    "print(\"type(engine):\", type(engine))\n",
    "# Calling engine.table_names() is deprecated.\n",
    "# table_names = engine.table_names()\n",
    "# Create an inspector object.\n",
    "inspector = sqlalchemy.inspect(engine)\n",
    "# Use an inspector to get the table names.\n",
    "table_names = inspector.get_table_names()\n",
    "print(table_names)\n",
    "# Use the inspector to get the view names.\n",
    "view_names = inspector.get_view_names()\n",
    "print(view_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadcb504-bd65-4b5c-bd5e-65857c98cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the table names using a query. The query also returns view names.\n",
    "# We reuse the sql1 query string from above.\n",
    "# The documentation says to use sqlalchemy.text(sql1) as the argument to\n",
    "# conn.execute(), which returns an iterator of type\n",
    "# sqlalchemy.engine.cursor.LegacyCursorResult.\n",
    "# With the context manager, the connection is automatically closed.\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(sqlalchemy.text(sql1))\n",
    "    print(\"result:\", result)\n",
    "    for row in result:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1fd63e-156b-4c19-a364-1675661cca26",
   "metadata": {},
   "source": [
    "#### Create a Database Engine and Get the Table Names (Exercises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f19641d-e7f4-4c26-a455-7720f93a1eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the recommended code.\n",
    "engine = sqlalchemy.create_engine('sqlite:///Chinook.sqlite', future=True)\n",
    "inspector = sqlalchemy.inspect(engine)\n",
    "table_names = inspector.get_table_names()\n",
    "print(table_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b8509f-736c-41d7-b559-751c8694f319",
   "metadata": {},
   "source": [
    "### Querying Relational Databases in Python\n",
    "The steps are:\n",
    "1) import packages and functions\n",
    "2) create the database engine\n",
    "3) connect to the engine\n",
    "4) query the database\n",
    "5) save the result set to a DataFrame\n",
    "6) close the connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83020c-7688-419e-bf8b-3e2c890438f3",
   "metadata": {},
   "source": [
    "#### Execute a Basic Query (Demonstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd6ff73-232e-49f8-87f4-bec27e88bd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code uses a context manager to manage the connection, as shown in\n",
    "# the demonstration.\n",
    "engine = sqlalchemy.create_engine(\"sqlite:///Northwind_small.sqlite\", future=True)\n",
    "with engine.connect() as conn:\n",
    "    # Since \"Order\" is a SQL keyword, we must quote it here.\n",
    "    result_set = conn.execute(sqlalchemy.text('SELECT * FROM \"Order\"'))\n",
    "    orders = pd.DataFrame(result_set.fetchall())\n",
    "    # Set the column names of the DataFrame.\n",
    "    orders.columns = result_set.keys()\n",
    "print(orders.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be861cd0-2114-4fc6-815c-562626fc6f91",
   "metadata": {},
   "source": [
    "#### Select Data from Specified Table Columns (Demonstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe05629-7cc2-4547-b715-3df80f157bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code reuses engine from above.\n",
    "# Select data from specified columns.\n",
    "with engine.connect() as conn:\n",
    "    result_set = conn.execute(sqlalchemy.text('SELECT Id, OrderDate, ShipName FROM \"Order\"'))\n",
    "    orders2 = pd.DataFrame(result_set.fetchmany(size=5))\n",
    "    # Fetch one row as follows:\n",
    "    # order2 = pd.DataFrame(result_set.fetchone())\n",
    "    orders2.columns = result_set.keys()\n",
    "print(orders2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408ebb27-e687-44f1-b5a5-69f73362ba30",
   "metadata": {},
   "source": [
    "#### Execute a Simple Query (Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ecf346-a0f8-4d11-a06b-f47f5c7d5777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all data from table Album.\n",
    "engine = sqlalchemy.create_engine('sqlite:///Chinook.sqlite', future=True)\n",
    "with engine.connect() as conn:\n",
    "    rs1 = conn.execute(sqlalchemy.text('SELECT * FROM Album'))\n",
    "    df1 = pd.DataFrame(rs1.fetchall())\n",
    "    df1.columns = rs1.keys()\n",
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4117e61-333a-4551-98f0-4efed8945b0e",
   "metadata": {},
   "source": [
    "#### Select Data from Specific Columns and Return Three Rows (Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787ad65f-f1f0-478c-8096-0a70e193aba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code reuses engine from above.\n",
    "# Select data from specific columns and return 3 rows.\n",
    "with engine.connect() as conn:\n",
    "    rs2 = conn.execute(sqlalchemy.text('SELECT LastName, Title FROM Employee'))\n",
    "    df2 = pd.DataFrame(rs2.fetchmany(size=3))\n",
    "    df2.columns = rs2.keys()\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d56662-2886-4e0a-8274-2c018ecda161",
   "metadata": {},
   "source": [
    "#### Select Data Using a WHERE Filter (Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede6aa3c-e926-457c-8ca6-7626794d5465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code reuses engine from above.\n",
    "# Filter the data using WHERE.\n",
    "with engine.connect() as conn:\n",
    "    rs3 = conn.execute(sqlalchemy.text('SELECT * FROM Employee WHERE EmployeeId >= 6'))\n",
    "    df3 = pd.DataFrame(rs3.fetchall())\n",
    "    df3.columns = rs3.keys()\n",
    "print(df3.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e313d42f-31bf-46f9-8161-7a5109e598f6",
   "metadata": {},
   "source": [
    "#### Select Data and Sort It (Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a61629-c575-4512-b02e-829b631998a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code reuses engine from above.\n",
    "# Select rows and sort them.\n",
    "with engine.connect() as conn:\n",
    "    rs4 = conn.execute(sqlalchemy.text('SELECT * FROM Employee ORDER BY BirthDate'))\n",
    "    df4 = pd.DataFrame(rs4.fetchall())\n",
    "    df4.columns = rs4.keys()\n",
    "print(df4.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acecfb4-abb9-4a8c-b376-e4ab2fd37b6d",
   "metadata": {},
   "source": [
    "### Querying Relational Databases Directly with pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4bcccf-40b8-4e3c-ac9b-95e7a434ad84",
   "metadata": {},
   "source": [
    "#### Use pandas Directly to Query a Database (Demonstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadc95d1-ca2e-4ed0-aeef-2c4663b5465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code reuses engine from above.\n",
    "# Use pandas directly to query a database.\n",
    "with engine.connect() as conn:\n",
    "    df5 = pd.read_sql_query(sqlalchemy.text('SELECT * FROM Employee ORDER BY BirthDate'), conn)\n",
    "    print(df5.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42de807-d0de-4a87-a8b8-155d9b6959dc",
   "metadata": {},
   "source": [
    "#### Use pandas to Obtain Database Data (Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58040f6d-eb3c-4b2e-b9a1-e6d88cbe5b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code reuses engine from above.\n",
    "# Use the new algorithm to select the data.\n",
    "# Use the old algorithm to select the data.\n",
    "# Check that the DataFrames contain the same data.\n",
    "with engine.connect() as conn:\n",
    "    df6 = pd.read_sql_query(sqlalchemy.text('SELECT * FROM Album'), conn)\n",
    "    print(df6.head())\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    rs7 = conn.execute(sqlalchemy.text('SELECT * FROM Album'))\n",
    "    df7 = pd.DataFrame(rs7.fetchall())\n",
    "    df7.columns = rs7.keys()\n",
    "print(df7.head())\n",
    "print()\n",
    "print(df6.equals(df7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543ed8a9-f19d-45ef-8f5d-f9be57ee3e66",
   "metadata": {},
   "source": [
    "#### Use pandas for a More Complex Query (Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12674ce6-021f-414a-b1a0-87897759a600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code reuses engine from above.\n",
    "# Execute a slightly more complex query using pandas.\n",
    "sql8 = \"\"\"\n",
    "    SELECT * \n",
    "    FROM Employee\n",
    "    WHERE EmployeeId >= 6\n",
    "    ORDER BY BirthDate\n",
    "\"\"\"\n",
    "with engine.connect() as conn:\n",
    "    df8 = pd.read_sql_query(sqlalchemy.text(sql8), conn)\n",
    "    print(df8.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac1f785-d3a6-4337-a8bc-715812a93940",
   "metadata": {},
   "source": [
    "### Advanced Querying: Exploiting Table Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fae8475-c6f2-4d4e-9ec9-3d5c01098208",
   "metadata": {},
   "source": [
    "#### Execute a Query with an Inner Join (Demonstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0c397f-59dc-4fd5-b807-015909024ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query is modified to work correctly with the Northwind_small.sqllite\n",
    "# database.\n",
    "engine = sqlalchemy.create_engine('sqlite:///Northwind_small.sqlite', future=True)\n",
    "sql9 = '''\n",
    "    SELECT\n",
    "        \"Order\".ID AS OrderId,\n",
    "        Customer.CompanyName\n",
    "    FROM\n",
    "        \"Order\"\n",
    "        INNER JOIN Customer\n",
    "            ON \"Order\".CustomerId = Customer.ID\n",
    "'''\n",
    "with engine.connect() as conn:\n",
    "    df9 = pd.read_sql_query(sqlalchemy.text(sql9), conn)\n",
    "    print(df9.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fd01f8-e35d-4575-a7db-fb166cde84a7",
   "metadata": {},
   "source": [
    "#### Execute a Query with an INNER JOIN (Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399efdee-b17c-4ea2-a34c-4a71d8d30281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain data from an inner join query and store it in a DataFrame.\n",
    "engine = sqlalchemy.create_engine('sqlite:///Chinook.sqlite', future=True)\n",
    "sql10 = \"\"\"\n",
    "    SELECT\n",
    "        Album.Title,\n",
    "        Artist.Name\n",
    "    FROM\n",
    "        Album\n",
    "        INNER JOIN Artist\n",
    "            ON Album.ArtistId = Artist.ArtistId\n",
    "\"\"\"\n",
    "with engine.connect() as conn:\n",
    "    rs10 = conn.execute(sqlalchemy.text(sql10))\n",
    "    df10 = pd.DataFrame(rs10.fetchall())\n",
    "    df10.columns = rs10.keys()\n",
    "print(df10.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34e2b89-6438-4a90-b77c-f2ab8e8b2c18",
   "metadata": {},
   "source": [
    "#### Filter an Inner Join (Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e192261-bf97-4265-a139-d568f36e2c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute a query with an inner join and a where clause.\n",
    "sql11 = '''\n",
    "    SELECT\n",
    "        *\n",
    "    FROM\n",
    "        PlaylistTrack\n",
    "        INNER JOIN Track\n",
    "            ON PlaylistTrack.TrackId = Track.TrackId\n",
    "    WHERE\n",
    "        Track.Milliseconds < 250000\n",
    "'''\n",
    "with engine.connect() as conn:\n",
    "    df11 = pd.read_sql_query(sqlalchemy.text(sql11), conn)\n",
    "    print(df11.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
